{
  
    
        "post0": {
            "title": "Introduction of time series forecasting with sktime",
            "content": "Main source: sktime documentation. . Author: Adelson de Araujo (a.dearaujo@utwente.nl) . Imports . ! pip install sktime --quiet ! pip install pmdarima --quiet . import numpy as np import matplotlib.pyplot as plt import sktime . Load some data . from sktime.datasets import load_macroeconomic, load_shampoo_sales, load_airline Y = load_macroeconomic() print(type(Y), type(Y.index)) Y.tail() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; &lt;class &#39;pandas.core.indexes.period.PeriodIndex&#39;&gt; . realgdp realcons realinv realgovt realdpi cpi m1 tbilrate unemp pop infl realint . Period . 2008Q3 13324.600 | 9267.7 | 1990.693 | 991.551 | 9838.3 | 216.889 | 1474.7 | 1.17 | 6.0 | 305.270 | -3.16 | 4.33 | . 2008Q4 13141.920 | 9195.3 | 1857.661 | 1007.273 | 9920.4 | 212.174 | 1576.5 | 0.12 | 6.9 | 305.952 | -8.79 | 8.91 | . 2009Q1 12925.410 | 9209.2 | 1558.494 | 996.287 | 9926.4 | 212.671 | 1592.8 | 0.22 | 8.1 | 306.547 | 0.94 | -0.71 | . 2009Q2 12901.504 | 9189.0 | 1456.678 | 1023.528 | 10077.5 | 214.469 | 1653.6 | 0.18 | 9.2 | 307.226 | 3.37 | -3.19 | . 2009Q3 12990.341 | 9256.0 | 1486.398 | 1044.088 | 10040.6 | 216.385 | 1673.9 | 0.12 | 9.6 | 308.013 | 3.56 | -3.44 | . from sktime.utils.plotting import plot_series realgdp = Y[&#39;realgdp&#39;] infl = Y[&#39;infl&#39;] fig, ax = plt.subplots(nrows=2, figsize=(15,6)) plot_series(realgdp, ax=ax[0]) plot_series(infl, ax=ax[1]) . &lt;AxesSubplot:ylabel=&#39;infl&#39;&gt; . Interpolation vs Regression vs Forecasting . Interpolation . import pandas as pd s = pd.Series([0, 1, 2, 3, 4, np.nan, 6]) ax = s.reset_index().plot.scatter(x=&#39;index&#39;, y=0) ax.set_xlim([0, 7]) ax.set_ylim([0, 7]) ax.fill_between([4,6], [7,7], alpha=0.2, color=&#39;orange&#39;); . s_interp = s.interpolate(method=&#39;linear&#39;) ax = s_interp.reset_index().plot.scatter(x=&#39;index&#39;, y=0) s_interp[[5]].plot(color=&#39;orange&#39;, marker=&#39;o&#39;, markersize=12, ax=ax) ax.set_xlim([0, 7]) ax.set_ylim([0, 7]) ax.fill_between([4,6], [7,7], alpha=0.2, color=&#39;orange&#39;) . &lt;matplotlib.collections.PolyCollection at 0x7f96f6d61610&gt; . s = pd.Series([0, 1, 2, 3, 4, np.nan]) ax = s.reset_index().plot.scatter(x=&#39;index&#39;, y=0) ax.set_xlim([0, 7]) ax.set_ylim([0, 7]) ax.fill_between([4,6], [7,7], alpha=0.2, color=&#39;orange&#39;); . s_interp = s.interpolate(method=&#39;linear&#39;, limit_direction=&#39;forward&#39;) ax = s_interp.reset_index().plot.scatter(x=&#39;index&#39;, y=0) s_interp[[5]].plot(color=&#39;orange&#39;, marker=&#39;o&#39;, markersize=12, ax=ax) ax.set_xlim([0, 7]) ax.set_ylim([0, 7]) ax.fill_between([4,6], [7,7], alpha=0.2, color=&#39;orange&#39;); . s_interp = s.interpolate(method=&#39;spline&#39;, order=1, limit_direction=&#39;forward&#39;) ax = s_interp.reset_index().plot.scatter(x=&#39;index&#39;, y=0) s_interp[[5]].plot(color=&#39;orange&#39;, marker=&#39;o&#39;, markersize=12, ax=ax) ax.set_xlim([0, 7]) ax.set_ylim([0, 7]) ax.fill_between([4,6], [7,7], alpha=0.2, color=&#39;orange&#39;); . Regression . . (Image source: A Friendly Introduction to Machine Learning) . Regression for time series involves finding/designing a good feature set ($X$) that predicts our target values ($Y$). This process is known in machine learning as feature engineering. . # ts -&gt; FeatureExtraction -&gt; X (features) X = FeatureExtraction.fit_transform(ts) # backwards information or not! . Lots of algorithms are widely used in the context of time series, including Linear Regression and tree-based ensembles such as Random Forest or Gradient Boosting. . from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import train_test_split from sklearn.pipeline import make_pipeline from sktime.forecasting.model_selection import temporal_train_test_split # X, y X_train, X_test, y_train, y_test = train_test_split(X, y) regressor = make_pipeline( # FeatureExtraction RandomForestRegressor(), ) regressor.fit(X_train, y_train) regressor.score(X_test, y_test) . Forecasting . (Image source: https://www.sktime.org/en/stable/examples/01_forecasting.html.) . Convert index to pd.DatetimeIndex: . y = y.to_timestamp(freq=&quot;M&quot;) y_train, y_test = temporal_train_test_split(y, test_size=36) . Transforming a regressor into forecaster: . from sktime.forecasting.base import ForecastingHorizon from sktime.forecasting.compose import make_reduction from sktime.forecasting.model_selection import temporal_train_test_split forecaster = make_reduction( regressor, scitype=&quot;time-series-regressor&quot;, window_length=12 ) forecaster.fit(y_train) fh = ForecastingHorizon(y_test.index) y_pred = forecaster.predict(fh) . Autocorrelation and stationarity . from sktime.utils.plotting import plot_correlations plot_correlations( realgdp, lags=36, alpha=0.05, pacf_method=&quot;ywadjusted&quot;, acf_title=&quot;Autocorrelation&quot;, pacf_title=&quot;Partial Autocorrelation&quot;, ); . plot_correlations( infl, lags=36, alpha=0.05, pacf_method=&quot;ywadjusted&quot;, acf_title=&quot;Autocorrelation&quot;, pacf_title=&quot;Partial Autocorrelation&quot;, ); . ARIMA as a strong baseline . ARIMA is an algorithm to find Autoregressive Integrated Moving-Average components and build a time series forecasting model. On its basic form, ARIMA has three main parameters to tune. How to find appropriate parameters for ARIMA (p, d, q)? The Box-Jenkins method was well-known as an approach to take the parameters from analysis on autocorrelation and stationarity. . p -&gt; Autoregressive components (a.k.a lags) | d -&gt; Integrative component (diff) | q -&gt; Moving average components (trend lags) | . But there are lots of other subtypes of ARIMA models, such as SARIMA that takes into account seasonality and many others. . If you find ARIMA an interesting algorithm and want know more about it, there are many great videos online. Here we will use the (famously on R) AutoARIMA method, restricting parameters to avoid overfitting. . from sktime.forecasting.arima import AutoARIMA from sktime.forecasting.naive import NaiveForecaster y = load_airline() # ARIMA forecaster = AutoARIMA(sp=12, suppress_warnings=True) forecaster.fit(y) print(f&quot;ARIMA info: n{forecaster.get_fitted_params()}&quot;) y_pred = forecaster.predict(fh=np.arange(1, 13)) # forecast the next 12 months at once # vs NaiveForecaster naive_forecaster = NaiveForecaster(strategy=&#39;last&#39;, sp=12) naive_forecaster.fit(y) y_pred_naive = naive_forecaster.predict(fh=np.arange(1, 13)) fig, ax = plt.subplots(nrows=2, figsize=(15,6)) ax[0].grid() plot_series(y, y_pred, ax=ax[0]) ax[1].grid() plot_series(y, y_pred_naive, ax=ax[1]); . ARIMA info: {&#39;ma.L1&#39;: -0.3634460835424699, &#39;ar.S.L12&#39;: -0.12386440506599077, &#39;ar.S.L24&#39;: 0.19105747581116445, &#39;sigma2&#39;: 130.4479953547119, &#39;order&#39;: (0, 1, 1), &#39;seasonal_order&#39;: (2, 1, 0, 12), &#39;aic&#39;: 1019.1780567487451, &#39;aicc&#39;: 1019.4955170662055, &#39;bic&#39;: 1030.6788460415498, &#39;hqic&#39;: 1023.8513413902231} . Other decision-making baselines . Interpretation Forecaster sktime . &quot;Tomorrow will be just like today&quot; | NaiveForecaster(strategy=&#39;last&#39;) | . &quot;Tomorrow will be close to the overall mean&quot; | NaiveForecaster(strategy=&#39;mean&#39;) | . &quot;Tomorrow will be the mean of the last three days&quot; | NaiveForecaster(strategy=&#39;mean&#39;, window_length=3) | . &quot;Next month will be as it was the same month of last year&quot; | NaiveForecaster(strategy=&#39;last&#39;, sp=12) | . Forecast evaluation workflow in a nutshell . from sktime.forecasting.model_selection import temporal_train_test_split from sktime.forecasting.base import ForecastingHorizon y = load_airline() y_train, y_test = temporal_train_test_split(y, test_size=36) # plotting for illustration plot_series(y_train, y_test, labels=[&quot;y_train&quot;, &quot;y_test&quot;]) print(f&quot;Train: {y_train.shape[0]} points nTest: {y_test.shape[0]} points&quot;) . Train: 108 points Test: 36 points . fh = ForecastingHorizon(y_test.index, is_relative=False) . from sktime.forecasting.compose import AutoEnsembleForecaster from sktime.forecasting.naive import NaiveForecaster from sktime.forecasting.trend import PolynomialTrendForecaster, STLForecaster from sktime.forecasting.exp_smoothing import ExponentialSmoothing from sklearn.metrics import mean_squared_error, r2_score forecasters = [ # (&quot;trend&quot;, STLForecaster(sp=12)), # (&quot;poly&quot;, PolynomialTrendForecaster(degree=1)), (&quot;expm&quot;, ExponentialSmoothing(trend=&quot;add&quot;)), # (&quot;naive&quot;, NaiveForecaster()), ] forecaster = AutoEnsembleForecaster(forecasters=forecasters) forecaster.fit(y=y_train, fh=fh) y_pred = forecaster.predict() # Compute performance metrics metrics = { &#39;mean_squared_error&#39;: mean_squared_error(y_test, y_pred), &#39;r2_score&#39;: r2_score(y_test, y_pred) } print(metrics) # plotting fig, ax = plt.subplots(figsize=(15,6)) plot_series(y_train, y_test, y_pred, labels=[&#39;y&#39;, &#39;y_test&#39;, &#39;y_pred&#39;], ax=ax); . {&#39;mean_squared_error&#39;: 7791.631326444609, &#39;r2_score&#39;: -0.27349496616259006} . Checking a strong baseline . from sktime.forecasting.arima import AutoARIMA from sktime.performance_metrics.forecasting import mean_absolute_percentage_error forecaster = AutoARIMA(sp=12, suppress_warnings=True) forecaster.fit(y_train) y_pred = forecaster.predict(fh = fh) plot_series(y_train, y_test, y_pred, labels=[&quot;y_train&quot;, &quot;y_test&quot;, &quot;y_pred&quot;]) # computing the forecast performance metrics = { &#39;mean_squared_error&#39;: mean_squared_error(y_test, y_pred), &#39;r2_score&#39;: r2_score(y_test, y_pred) } print(metrics) . {&#39;mean_squared_error&#39;: 489.8359037668583, &#39;r2_score&#39;: 0.9199392872227382} . forecasters = [ (&quot;trend&quot;, STLForecaster(12)), # (&quot;poly&quot;, PolynomialTrendForecaster(degree=1)), (&quot;expm&quot;, ExponentialSmoothing(trend=&quot;add&quot;)), # (&quot;naive&quot;, NaiveForecaster(strategy=&quot;last&quot;, sp=12)), ] forecaster = AutoEnsembleForecaster(forecasters=forecasters) forecaster.fit(y_train) y_pred = forecaster.predict(fh = fh) plot_series(y_train, y_test, y_pred, labels=[&quot;y_train&quot;, &quot;y_test&quot;, &quot;y_pred&quot;]) # computing the forecast performance metrics = { &#39;mean_squared_error&#39;: mean_squared_error(y_test, y_pred), &#39;r2_score&#39;: r2_score(y_test, y_pred) } print(metrics) . {&#39;mean_squared_error&#39;: 619.8883331411145, &#39;r2_score&#39;: 0.8986830050391579} . Take aways . Interpolation, Regression and Forecasting are techniques that use diferent methods to make predictions; | Our world is chaotic thus your time series forecasting task may be more complex (multivariate, etc); | Model evaluation is crucial, including baseline analysis; | Validation may require the support of a domain expert that can interpret the results; | Applications can build expert systems (agents) that use predictions to act automaticaly (predictive controllers). | . Themes for additional discussion . Model-centric improvements: Machine learning, Deep learning, AutoML; | Ethics, transparency, reprodutibility and interpretability; | ... | .",
            "url": "https://adaj.github.io/blog/2022/06/06/Introduction-of-time-series-forecasting-with-sktime.html",
            "relUrl": "/2022/06/06/Introduction-of-time-series-forecasting-with-sktime.html",
            "date": " • Jun 6, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Topic modeling on scientific literature",
            "content": "Author: Adelson de Araujo (a.dearaujo@utwente.nl) . Topic modeling algorithms, according to Blei (2012), are statistical techniques that examine the words of the original texts to find the themes that run through them, how those themes are connected, and how they change over time.   . One of the reasons for their wide use is that all these algorithms can be applied to a corpus of unlabeled texts, and discover latent features to reflect themes or topics that occur in the corpus.  . The current most widely used topic modeling algorithms include Latent Dirichlet Allocation (LDA, Blei et al., 2003), Probabilistic Latent Semantic Analysis (pLSA, Hofmann, 1999) and its extension Non-negative Matrix Factorization (NMF, Lee and Seung, 1999). . The current use of topic modeling is in a variety of fields. In computational linguistics and natural language processing, topic modeling is used to discover hidden topics in documents. In text mining, the clustering of text documents into groups of similar topics is of interest. . In an ambitious scenario, some might suggest that topic modeling can be used to find hidden or latent social structures in the text of online forums. But more modestly, I think we can walk together with this notebook through the introductory steps needed to implement topic modeling analysis on scientific texts. . With the help of the definition stated in the beginning, we chose to analyze the following factors: . What are some of the literature&#39;s most popular topics?  | What links those topics together?  | How frequently were those topics covered over time? | . Approach . The code below provides a gist on how to leverage topic modeling on scientific literature (PDFs). . I recommend doing this approach before conducting a systematic or scoping literature review in order to increase the eyesight of what you as a researcher should expect to see as topics that surround the papers under investigation. . To illustrate the procedure, I collected some papers in three well-known search engines for scientific articles, namely Web of Science, Scopus and IEEE. In /data, you can find the queries used in each search engine, as well as the exported metadata. I cannot provide the results PDFs due to copyright issues. . Query phrase: ( ( algorithm* OR automat* OR bot$ OR &quot;artificial intelligence&quot; ) AND ( financ* OR trading ) AND ( &quot;social media&quot; OR twitter OR facebook OR youtube OR reddit OR telegram ) AND ( manipulat* OR disinformation ) ) . If you use this code or find it interesting, please cite us! . @article{pohl2022social, title={Social bots spreading disinformation about finance: research trends, and ethical challenges}, author={Pohl, Janina and Griesbach, Marie and Samiei, Alireza and Araujo, Adelson}, journal={In press}, volume={0}, number={0}, pages={0}, year={2022}, publisher={In press} } . Feel free to leave a comment with a suggestion. . Imports . PDF_FOLDER = &#39;./data/pdf&#39; MASK_IMAGE_PATH = &#39;./images/cloud.png&#39; OUTPUT_FOLDER = &#39;./outputs&#39; SEED = 0 N_TOPICS = 6 N_WORDS_PER_TOPIC = 50 N_WORDS_HIGHLIGHT_WORDCLOUD = 5 with open(&#39;./utils/stop_words_EN.txt&#39;, &#39;r&#39;) as f: STOP_WORDS = f.read().split(&#39; n&#39;) . Load documents . documents, filenames, years = read_pdfs(PDF_FOLDER) print(f&#39;{len(documents)} PDFs loaded as text.&#39;) . 19 PDFs loaded as text. . Preprocessing . %%time nlp = spacy.load(&quot;en_core_web_sm&quot;) docs = [] # Filter lemmas for tokens in [&#39;ADJ&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;PROPN&#39;] for doc in nlp.pipe(documents): filtered = &#39; &#39;.join([token.lemma_ for token in doc if token.pos_ in [&#39;ADJ&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;PROPN&#39;]]) docs.append(filtered) . CPU times: user 37.4 s, sys: 10.5 s, total: 47.9 s Wall time: 1min 9s . Topic model . pipe = Pipeline([ (&#39;tfidf&#39;, TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words=STOP_WORDS, token_pattern=r&#39;(?u) b[A-Za-z]+ b&#39;, ngram_range=(1, 3))), (&#39;topic_model&#39;, NMF(N_TOPICS, max_iter=1000, random_state=1)) ]) . pipe.fit(docs) . /Users/adelsondias/miniconda3/envs/ca-prototype/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:289: FutureWarning: The &#39;init&#39; value, when &#39;init=None&#39; and n_components is less than n_samples and n_features, will be changed from &#39;nndsvd&#39; to &#39;nndsvda&#39; in 1.1 (renaming of 0.26). warnings.warn( . Pipeline(steps=[(&#39;tfidf&#39;, TfidfVectorizer(max_df=0.9, max_features=1000, min_df=2, ngram_range=(1, 3), stop_words=[&#39;ll&#39;, &#39;tis&#39;, &#39;twas&#39;, &#39;ve&#39;, &#39;000&#39;, &#39;00&#39;, &#39;a&#39;, &#39;as&#39;, &#39;able&#39;, &#39;ableabout&#39;, &#39;about&#39;, &#39;above&#39;, &#39;abroad&#39;, &#39;abst&#39;, &#39;accordance&#39;, &#39;according&#39;, &#39;accordingly&#39;, &#39;across&#39;, &#39;act&#39;, &#39;actually&#39;, &#39;ad&#39;, &#39;added&#39;, &#39;adj&#39;, &#39;adopted&#39;, &#39;ae&#39;, &#39;af&#39;, &#39;affected&#39;, &#39;affecting&#39;, &#39;affects&#39;, &#39;after&#39;, ...], token_pattern=&#39;(?u) b[A-Za-z]+ b&#39;)), (&#39;topic_model&#39;, NMF(max_iter=1000, n_components=6, random_state=1))]) . Analysis . a. What are some prominent topics in the literature? . topic_words, words_importance = get_topic_words(pipe, N_WORDS_PER_TOPIC) tw = pd.DataFrame(topic_words).melt(var_name=&#39;topic_id&#39;, value_name=&#39;word&#39;) wi = pd.DataFrame(words_importance).melt(var_name=&#39;topic_id&#39;, value_name=&#39;importance&#39;) topic_word_importance = pd.concat([tw,wi[&#39;importance&#39;]], axis=1) topic_word_importance = topic_word_importance.set_index([&#39;topic_id&#39;, &#39;word&#39;]) topic_word_importance.sample(5) . importance . topic_id word . 0 price 0.145109 | . 2 detection 0.141785 | . 3 human 0.149593 | . 5 source 0.046277 | . 3 download 0.089857 | . TOPIC_LABELS = { 1: &#39;Sentiment analysis&#39;, 2: &#39;Bot detection&#39;, 3: &#39;Campaign promotion&#39;, 4: &#39;Disinformation and privacy&#39;, 5: &#39;Pumps and dumps&#39;, 6: &#39;Rumor spreading&#39; } . docs_topics = pd.DataFrame(pipe.transform(docs), columns=list(TOPIC_LABELS.values()), index=filenames) docs_topics . Sentiment analysis Bot detection Campaign promotion Disinformation and privacy Pumps and dumps Rumor spreading . Ibrahim - 2021 - Forecasting the early market movement in bitcoin using twitter&#39;s sentiment analysis An ensemble-based prediction model.pdf 0.469492 | 0.000000 | 0.000000 | 0.041820 | 0.041896 | 0.000000 | . Lange, Kettani - 2019 - On Security Threats of Botnets to Cyber Systems.pdf 0.000000 | 0.251676 | 0.000000 | 0.267881 | 0.000000 | 0.000000 | . Sela et al - 2020 - Using connected accounts to enhance information spread in social networks.pdf 0.000000 | 0.140555 | 0.015975 | 0.000000 | 0.069932 | 0.620700 | . Kudugunta - 2018 - Deep neural networks for bot detection.pdf 0.076697 | 0.580714 | 0.072897 | 0.000000 | 0.000000 | 0.000000 | . Geckil et al. - 2018 - A clickbait detection method on news sites.pdf 0.085922 | 0.000000 | 0.183729 | 0.326146 | 0.035637 | 0.103153 | . Tardelli et al. - 2020 - Characterizing Social Bots Spreading Financial Disinformation.pdf 0.028358 | 0.750603 | 0.000000 | 0.000000 | 0.000000 | 0.060125 | . Khaund et al. - 2021 - Social Bots and Their Coordination During Online Campaigns A Survey.pdf 0.000000 | 0.726069 | 0.000000 | 0.095858 | 0.030996 | 0.000000 | . Mahmood - 2019 - Antidatamining framework - Better privacy on Online Social Networks and Beyond.pdf 0.000000 | 0.007458 | 0.000000 | 0.642140 | 0.005048 | 0.000000 | . Dogan et al. - 2020 - Speculator and Influencer Evaluation in Stock Market by Using Social Media.pdf 0.509662 | 0.000000 | 0.000000 | 0.078509 | 0.000000 | 0.057278 | . Nizzoli et al. - 2020 - Charting the Landscape of Online Cryptocurrency Manipulation.pdf 0.000000 | 0.054760 | 0.000000 | 0.000000 | 0.826464 | 0.000000 | . Varol et al. - 2017 - Early detection of promoted campaigns on social media.pdf 0.003892 | 0.009413 | 0.866455 | 0.000000 | 0.000049 | 0.048419 | . Golmohammadi, Zaiane - 2017 - Sentiment Analysis on Twitter to Improve Time Series Contextual Anomaly Detection for Detecting Stock Market Manipulation.pdf 0.427723 | 0.000000 | 0.151739 | 0.000000 | 0.000000 | 0.000000 | . Tardelli et al - 2022 - Detecting inorganic financial compaigns on Twitter.pdf 0.047495 | 0.492811 | 0.236611 | 0.000000 | 0.004616 | 0.168862 | . Fernandez Vilas, Diaz Redondo, Lorenzo Garcia - 2020 - The Irruption of Cryptocurrencies into Twitter Cashtags A Classifying Solution.pdf 0.410586 | 0.032230 | 0.000000 | 0.000000 | 0.064085 | 0.092800 | . Mirtaheri et al. - 2021 - Identifying and Analyzing Cryptocurrency Manipulations in Social Media.pdf 0.071860 | 0.000000 | 0.009478 | 0.014403 | 0.803376 | 0.000000 | . Ferrara et al. - 2016 - Detection of Promoted Social Media Campaigns.pdf 0.000000 | 0.000000 | 0.876062 | 0.000000 | 0.000000 | 0.000000 | . Isle, Smith - 2019 - Real World Examples Suggest a Path to Automated Mitigation of Disinformation.pdf 0.009876 | 0.000000 | 0.000000 | 0.699743 | 0.000000 | 0.000000 | . Land, Aronson - 2020 - Human Rights and Technology New Challenges for Justice and Accountability.pdf 0.000000 | 0.000000 | 0.000000 | 0.544316 | 0.000000 | 0.079105 | . Majumdar, Bose - 2018 - Detection of financial rumors using big data analytics the case of the Bombay Stock Exchange.pdf 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.054615 | . sns.boxplot(data=docs_topics.melt(), y=&#39;variable&#39;, x=&#39;value&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f8a386ab790&gt; . b. How those themes are connected to each other? . Sentiment analysis Bot detection Campaign promotion Disinformation and privacy Pumps and dumps Rumor spreading . Sentiment analysis 0.0 | 0.0 | 2.0 | 1.0 | 0.0 | 2.0 | . Bot detection 0.0 | 0.0 | 1.0 | 2.0 | 0.0 | 2.0 | . Campaign promotion 2.0 | 1.0 | 0.0 | 1.0 | 0.0 | 2.0 | . Disinformation and privacy 1.0 | 2.0 | 1.0 | 0.0 | 0.0 | 1.0 | . Pumps and dumps 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . Rumor spreading 2.0 | 2.0 | 2.0 | 1.0 | 0.0 | 0.0 | . Node sizes: {&#39;Sentiment analysis&#39;: 5000, &#39;Bot detection&#39;: 6000, &#39;Campaign promotion&#39;: 5000, &#39;Disinformation and privacy&#39;: 6000, &#39;Pumps and dumps&#39;: 2000, &#39;Rumor spreading&#39;: 5000} Tip: Draw this in another software (e.g. draw.io) if you wish a prettier representation. . c. How frequent those themes were addressed over time? . References . Blei, D. M., Ng, A. Y., &amp; Jordan, M. I. (2003). Latent dirichlet allocation. Journal of machine Learning research, 3(Jan), 993-1022. | Blei, D. M. (2012). Probabilistic topic models. Communications of the ACM, 55(4), 77-84. | Hofmann, T. (1999). Probabilistic latent semantic indexing. In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval (pp. 50-57). | Lee, D. D., &amp; Seung, H. S. (1999). Learning the parts of objects by non-negative matrix factorization. Nature, 401(6755), 788-791. | .",
            "url": "https://adaj.github.io/blog/2022/01/07/Topic-modeling-on-scientific-literature.html",
            "relUrl": "/2022/01/07/Topic-modeling-on-scientific-literature.html",
            "date": " • Jan 7, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Please visit my website. If you wish, contact me directly via a.dearaujo@utwente.nl. .",
          "url": "https://adaj.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://adaj.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}