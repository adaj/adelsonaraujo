{
  
    
        "post0": {
            "title": "Introduction of zero-shot classification w/ chats",
            "content": ". In this notebook, I would like to make a rapid introduction to zero-shot classification for practitioners. We will cover what it is and explore a use case. . This is not a comprehensive tutorial, nor does it discuss efficient approaches. Still, it&#39;s more like a taste of zero-shot classification in practice for those with no experience with it. . After being positively surprised with my toy examples, I prepared this notebook. I wanted to share what I see as a potentially game-changing technique to apply machine learning when ground-truth labels are unavailable or are costly to be collected. . In conclusion, I mention some takeaways that I see on how zero-shot classification cal help in pipelines of feature engineering since you can use its output of intermediary (perhaps even self-explainable) variables to other systems. . Zero-shot classification, what is that?! . To be more accurate, zero-shot classification is referred to in the ML literature as zero-shot learning. Up until now, I have used classification to avoid an initial confusion with the &quot;learning&quot; term. In the toy code below, I explore its use in inference time, which I referred to as classification. . But, first, a few words on what is zero-shot learning. Ian Goodfellow&#39;s wrote in Quora the following answer. . Zero-shot learning is being able to solve a task despite not having received any training examples of that task. . He answered the question quite some time ago, so this is not a new thing. . The most common example you will see out there about zero-shot learning is the zebra example. Suppose you only trained your model with horses. Now you want to classify a zebra, but your model doesn&#39;t know about it. But if you are able to say to the model that &quot;zebra&quot;s (a new label) are striped and horse-like (features), a model able to perform zero-shot learning can correctly identify that zebra. . Joe Davison wrote recently about zero-shot learning, where he greatly explains successes in the field of transfer learning that allowed models to perform surprisingly well in several use cases. . As far as I can see, zero-shot classification (inference) has been more materialized recently with advances in transfer learning and is particularly accessible through HuggingFace&#39;s transformer library. New architectures are being proposed and because there are some interesting datasets where bigger models can be pre-trained. More people are using pre-trained models and engaging in transfer learning pipelines. . Okay, but labeling without training samples is really doable? . Zero-shot learning is a particular form of transfer learning. That there are different ways to do the job, and techniques vary in computer vision and NLP. Of course, I still want to see more studies discussing interrater reliability with these kind of tools in a diverse set of scenarios to have a stronger argument on using it &quot;in the wild&quot;. . In this notebook, we will walk through how you can put your hands in some unlabeled text data and label it automatically using models available from HuggingFace&#39;s transformers. . I don&#39;t have too much time. Can you show me this quickly? . Sure, I understand. First, I will show a small demonstration. . If you want to go through a second example, I will use some data from DOTA-2 chats to classify them as one of the following candidate_labels = [&#39;chitchat&#39;, &#39;game features&#39;, &#39;coordination&#39;, &#39;toxic offense&#39;, &#39;gender discrimination&#39;, &#39;religious intolerance&#39;, &#39;racism&#39;]. This data is in Russian, so we have a translation step in between that we may loose some information. Also we are not carrying too much in preprocessing steps, but they are indeed required for more serious projects. Also, it seems that there are zero-shot models in a few other languages available out there, such as French, Spanish, German, even Russian. See a list from HuggingFace models. . Example 1: Reasoning or personal impression? . This is an example taken from Fiacco &amp; Rosé (2018). . Suppose you want to classify a text as a causal reasoning, a evaluation reasoning or a personal impression. . If someone says . &quot;Use of coal increases pollution&quot;, we expect the label to be causal reasoning. | &quot;Use of wind power may not be reliable throughtout the year&quot;, we expect the label to be evaluation reasoning. | &quot;I prefer coal power&quot;, we expect the label to be personal impression. | . Of course, these are not completely exclusive classes and could be better conceived to be exclusive, but suppose these are exclusive. . We can use a pre-trained model that is able to perform zero-shot learning and generate labels without any previous training data: . Imports and utilities . import os import random import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from transformers import AutoTokenizer, pipeline from transformers import AutoModelForSeq2SeqLM from transformers import AutoModelForSequenceClassification def zeroshot_classifier(): # Only works with English text tokenizer = AutoTokenizer .from_pretrained(&quot;facebook/bart-large-mnli&quot;) model = AutoModelForSequenceClassification .from_pretrained(&quot;facebook/bart-large-mnli&quot;) return pipeline(task=&#39;zero-shot-classification&#39;, model=model, tokenizer=tokenizer) plt.rcParams.update({&#39;font.size&#39;: 18, &quot;font.family&quot;: &quot;Times&quot;}) . Load a pre-trained zero-shot classifier and define candidate_labels . pipe = zeroshot_classifier() some_texts = [&quot;Use of coal increases pollution&quot;, &quot;Use of wind power may not be reliable throughout the year&quot;, &quot;I prefer coal power&quot;] candidate_labels = [&#39;causal reasoning&#39;, &#39;evaluation reasoning&#39;, &#39;personal impression&#39;] predictions = pipe(some_texts, candidate_labels=candidate_labels) . Check the outputs . Why does this work? . I suggest you read the description of the model we are using here. They answer this question very clearly, but you must have known what NLI is. . If you want to read my own (shorter) explanation of their explanation, here it is: . NLI stands for Natural Language Inference, and it refers to a particular classification task. Suppose two pieces of texts, a premise, and a hypothesis. The option of labels are entailment (when the hypothesis confirms the premise), contradiction (when the hypothesis denies the premise), and neutral. Check some examples [here]. There are some NLI datasets available; for example, SNLI and MultiNLI are the most famous ones I know of. Some people also call NLI&#39;s task the entailment classification task. . The MultiNLI dataset is enormous and enables robust algorithm architectures to produce very good models that can be transferred to other semantically similar tasks. . But what it has to do with zero-shot? . Suppose the model does a great job at the entailment task. Given our set of three candidate_labels, zero-shot learning can be leveraged in the following way. Take the input text as the premise. For each ith candidate_labels, turn the i-th candidate label into &quot;This sentence is about {i}&quot; as the hypothesis. Predict the entailment of each hypothesis and transform the output to generate a probability for each candidate_label. . Let&#39;s go through another example to see it in action in a more challenging context. . Example 2: DOTA-2 in-game chats . Load the data . df = pd.read_csv(&#39;~/Downloads/dota2_chat_messages.csv&#39;, nrows=100) df[&#39;text&#39;] = df[&#39;text&#39;].fillna(&#39;&#39;) print(&#39;Mean length of text&#39;, df[&#39;text&#39;].apply(lambda x: len(x)).mean()) print(df.head(15)) . Mean length of text 11.83 match time slot text 0 0 1005.12122 9 ладно гг 1 0 1005.85442 9 изи 2 0 1008.65372 9 од 3 0 1010.51992 9 ебаный 4 0 1013.91912 9 мусор на войде 5 0 1800.31402 9 мусор 6 0 1801.71882 9 на войде 7 0 1802.98982 9 репорт 8 0 1808.40822 9 100% 9 1 -131.14018 0 twitch.tv/rage_channel 10 1 -121.60481 0 https://www.twitch.tv/rage_channel 11 1 244.47367 7 2 даша подряд 12 1 249.93900 7 баша 13 1 255.00443 4 где даша? 14 1 261.20293 4 даша домой . %%time df_sample = df.sample(30) . CPU times: user 1.14 ms, sys: 827 µs, total: 1.97 ms Wall time: 4.54 ms . Translate to English . def translator(src: str, dest: str): src = src.lower() dest = dest.lower() tokenizer = AutoTokenizer .from_pretrained(f&quot;Helsinki-NLP/opus-mt-{src}-{dest}&quot;) model = AutoModelForSeq2SeqLM .from_pretrained(f&quot;Helsinki-NLP/opus-mt-{src}-{dest}&quot;) return pipeline(task=&#39;translation&#39;, model=model, tokenizer=tokenizer) translate = translator(&#39;ru&#39;, &#39;en&#39;) . %%time translated_text = translate([t[:100] for t in list(df_sample[&#39;text&#39;])]) df_sample[&#39;text_en&#39;] = [t[&#39;translation_text&#39;] for t in translated_text] . CPU times: user 2min 15s, sys: 1min 19s, total: 3min 35s Wall time: 3min 37s . df_sample.head(10) . match time slot text text_en . 74 4 | 135.73355 | 9 | мде | Yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah... | . 53 3 | 2250.84410 | 4 | + | + | . 48 3 | 2199.00090 | 7 | WAC | WAC | . 63 4 | -4.46558 | 8 | ну и?))) | What do you mean? ))))) | . 0 0 | 1005.12122 | 9 | ладно гг | Okay. | . 31 2 | 1996.39360 | 8 | idiot drow | idiot drow | . 16 1 | 689.59830 | 6 | даун с 1 тычки забашил | Down with 1 pumpkins. | . 61 4 | -47.32178 | 8 | это ты так думаешь))) | That&#39;s what you think))) | . 62 4 | -38.79053 | 1 | профиль глянь :D | Look at the profile:D | . 69 4 | 38.15735 | 3 | ласт пиком троля | The power of the spade of the throne | . Load a pre-trained zero-shot classifier and define candidate_labels . candidate_labels = [&#39;chitchat&#39;, &#39;game features&#39;, &#39;coordination&#39;, &#39;toxic offense&#39;, &#39;gender discrimination&#39;, &#39;religious intolerance&#39;, &#39;racism&#39;] pipe = zeroshot_classifier() . %%time predictions = pipe(list(df_sample[&#39;text_en&#39;]), candidate_labels=candidate_labels) predictions[0] . CPU times: user 4min 18s, sys: 35.2 s, total: 4min 53s Wall time: 5min 31s . {&#39;sequence&#39;: &#39;Yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah.&#39;, &#39;labels&#39;: [&#39;chitchat&#39;, &#39;coordination&#39;, &#39;game features&#39;, &#39;gender discrimination&#39;, &#39;religious intolerance&#39;, &#39;toxic offense&#39;, &#39;racism&#39;], &#39;scores&#39;: [0.4074071943759918, 0.218764066696167, 0.18153725564479828, 0.05758378282189369, 0.047326117753982544, 0.044934190809726715, 0.04244731739163399]} . Check the outputs . match time slot text text_en label . 74 4 | 135.73355 | 9 | мде | Yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah... | chitchat | . 53 3 | 2250.84410 | 4 | + | + | coordination | . 48 3 | 2199.00090 | 7 | WAC | WAC | coordination | . 63 4 | -4.46558 | 8 | ну и?))) | What do you mean? ))))) | chitchat | . 0 0 | 1005.12122 | 9 | ладно гг | Okay. | chitchat | . 31 2 | 1996.39360 | 8 | idiot drow | idiot drow | toxic offense | . 16 1 | 689.59830 | 6 | даун с 1 тычки забашил | Down with 1 pumpkins. | coordination | . 61 4 | -47.32178 | 8 | это ты так думаешь))) | That&#39;s what you think))) | chitchat | . 62 4 | -38.79053 | 1 | профиль глянь :D | Look at the profile:D | chitchat | . 69 4 | 38.15735 | 3 | ласт пиком троля | The power of the spade of the throne | coordination | . 2 0 | 1008.65372 | 9 | од | - Yeah, yeah, yeah, yeah, yeah, yeah, yeah, ye... | chitchat | . 13 1 | 255.00443 | 4 | где даша? | Where&#39;s the lady? | chitchat | . 12 1 | 249.93900 | 7 | баша | Hey, bastard. | chitchat | . 96 4 | 1914.43022 | 5 | ах ты крыса мелкая | You little rat. | toxic offense | . 47 3 | 1029.98180 | 2 | g | g | coordination | . 39 2 | 2266.44690 | 4 | EZ | EZ | coordination | . 52 3 | 2243.75330 | 1 | репорт батрайдер | Report on the battery | toxic offense | . 71 4 | 99.94226 | 1 | боже | Oh, my God. | toxic offense | . 76 4 | 228.61087 | 1 | у нас фидер | We&#39;ve got a fider. | coordination | . 9 1 | -131.14018 | 0 | twitch.tv/rage_channel | Twitch.tv/range_channel | chitchat | . 25 2 | 1248.42850 | 0 | yes dog | yes dog | coordination | . 18 1 | 700.72893 | 0 | https://www.twitch.tv/rage_channel | https://www.twitch.tv/range_channel | coordination | . 98 4 | 2116.45172 | 6 | .l. | .l. | coordination | . 27 2 | 1281.95360 | 4 | HAHAH | HAHAH | chitchat | . 4 0 | 1013.91912 | 9 | мусор на войде | There&#39;s garbage in the door. | toxic offense | . 49 3 | 2212.78120 | 3 | vac d o l b a e b | vac d o l b a e b | coordination | . 46 3 | 390.23810 | 6 | найс залагало | I&#39;m sorry, but it&#39;s been a long time since I&#39;v... | chitchat | . 54 3 | 2289.84370 | 0 | )) | ))) | coordination | . 8 0 | 1808.40822 | 9 | 100% | 100% | coordination | . 92 4 | 916.87612 | 8 | я мать ебал доты | I fucked my mother. | toxic offense | . Concluding remarks . In case you want to know more about zero-shot learning, I encourage you to go through the following material: . https://joeddav.github.io/blog/2020/05/29/ZSL.html | https://arxiv.org/abs/1909.00161 | https://www.deeplearningbook.org/contents/representation.html (Section 15.2) | https://www.aaai.org/Papers/AAAI/2008/AAAI08-132.pdf | . As a takeaway, I think a well-designed zero-shot classifier (with suitable candidate labels) can be a game-changing tool for several AI projects. . One use case is, for example, &quot;expert systems,&quot; where you generate these output probabilities for classes that you understand as intermediary features that you provide to a rule-based decision-making mechanism. Then you can write things like &quot;if causal_reasoning is high, do the action A; if evaluation_reasoning is high, do the action B.&quot; There has been some research opportunity with zero-shot learning as a feature engineering step because of this kind of system. Interestingly, one can automate understandable features computed from black-box models. Also, if you build candidate_labels using some theory is great because you have more evidence to back up design choices. . The main issue I see with this is how we can evaluate if the zero-shot labels are suitable. Well, one way to measure is by letting the model compute several labels, and then you or another person can label manually. With your labels and the zero-shot ones, you can use Cohen&#39;s Kappa as your agreement level. That, however, is not scalable when you are testing several candidate_labels. . If you see interesting use cases for zero-shot learning or want to interact/discuss this notebook, please comment below!! .",
            "url": "https://adaj.github.io/blog/2021/11/27/Introduction-of-zero-shot-classification-w-chats.html",
            "relUrl": "/2021/11/27/Introduction-of-zero-shot-classification-w-chats.html",
            "date": " • Nov 27, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://adaj.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://adaj.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}