{
  
    
        "post0": {
            "title": "Introduction of time series forecasting with [`sktime`](https://github.com/alan-turing-institute/sktime)",
            "content": "Main source: sktime documentation. . Author: Adelson de Araujo (a.dearaujo@utwente.nl) . Imports . ! pip install sktime --quiet ! pip install pmdarima --quiet . import numpy as np import matplotlib.pyplot as plt import sktime . Load some data . from sktime.datasets import load_macroeconomic, load_shampoo_sales, load_airline Y = load_macroeconomic() print(type(Y), type(Y.index)) Y.tail() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; &lt;class &#39;pandas.core.indexes.period.PeriodIndex&#39;&gt; . realgdp realcons realinv realgovt realdpi cpi m1 tbilrate unemp pop infl realint . Period . 2008Q3 13324.600 | 9267.7 | 1990.693 | 991.551 | 9838.3 | 216.889 | 1474.7 | 1.17 | 6.0 | 305.270 | -3.16 | 4.33 | . 2008Q4 13141.920 | 9195.3 | 1857.661 | 1007.273 | 9920.4 | 212.174 | 1576.5 | 0.12 | 6.9 | 305.952 | -8.79 | 8.91 | . 2009Q1 12925.410 | 9209.2 | 1558.494 | 996.287 | 9926.4 | 212.671 | 1592.8 | 0.22 | 8.1 | 306.547 | 0.94 | -0.71 | . 2009Q2 12901.504 | 9189.0 | 1456.678 | 1023.528 | 10077.5 | 214.469 | 1653.6 | 0.18 | 9.2 | 307.226 | 3.37 | -3.19 | . 2009Q3 12990.341 | 9256.0 | 1486.398 | 1044.088 | 10040.6 | 216.385 | 1673.9 | 0.12 | 9.6 | 308.013 | 3.56 | -3.44 | . from sktime.utils.plotting import plot_series realgdp = Y[&#39;realgdp&#39;] infl = Y[&#39;infl&#39;] fig, ax = plt.subplots(nrows=2, figsize=(15,6)) plot_series(realgdp, ax=ax[0]) plot_series(infl, ax=ax[1]) . &lt;AxesSubplot:ylabel=&#39;infl&#39;&gt; . Interpolation vs Regression vs Forecasting . Interpolation . import pandas as pd s = pd.Series([0, 1, 2, 3, 4, np.nan, 6]) ax = s.reset_index().plot.scatter(x=&#39;index&#39;, y=0) ax.set_xlim([0, 7]) ax.set_ylim([0, 7]) ax.fill_between([4,6], [7,7], alpha=0.2, color=&#39;orange&#39;); . s_interp = s.interpolate(method=&#39;linear&#39;) ax = s_interp.reset_index().plot.scatter(x=&#39;index&#39;, y=0) s_interp[[5]].plot(color=&#39;orange&#39;, marker=&#39;o&#39;, markersize=12, ax=ax) ax.set_xlim([0, 7]) ax.set_ylim([0, 7]) ax.fill_between([4,6], [7,7], alpha=0.2, color=&#39;orange&#39;) . &lt;matplotlib.collections.PolyCollection at 0x7f96f6d61610&gt; . s = pd.Series([0, 1, 2, 3, 4, np.nan]) ax = s.reset_index().plot.scatter(x=&#39;index&#39;, y=0) ax.set_xlim([0, 7]) ax.set_ylim([0, 7]) ax.fill_between([4,6], [7,7], alpha=0.2, color=&#39;orange&#39;); . s_interp = s.interpolate(method=&#39;linear&#39;, limit_direction=&#39;forward&#39;) ax = s_interp.reset_index().plot.scatter(x=&#39;index&#39;, y=0) s_interp[[5]].plot(color=&#39;orange&#39;, marker=&#39;o&#39;, markersize=12, ax=ax) ax.set_xlim([0, 7]) ax.set_ylim([0, 7]) ax.fill_between([4,6], [7,7], alpha=0.2, color=&#39;orange&#39;); . s_interp = s.interpolate(method=&#39;spline&#39;, order=1, limit_direction=&#39;forward&#39;) ax = s_interp.reset_index().plot.scatter(x=&#39;index&#39;, y=0) s_interp[[5]].plot(color=&#39;orange&#39;, marker=&#39;o&#39;, markersize=12, ax=ax) ax.set_xlim([0, 7]) ax.set_ylim([0, 7]) ax.fill_between([4,6], [7,7], alpha=0.2, color=&#39;orange&#39;); . Regression . . Regression for time series involves finding/designing a good feature set ($X$) that predicts our target values ($Y$). This process is known in machine learning as feature engineering. . # ts -&gt; FeatureExtraction -&gt; X (features) X = FeatureExtraction.fit_transform(ts) # backwards information or not! . Lots of algorithms are widely used in the context of time series, including Linear Regression and tree-based ensembles such as Random Forest or Gradient Boosting. . from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import train_test_split from sklearn.pipeline import make_pipeline from sktime.forecasting.model_selection import temporal_train_test_split # X, y X_train, X_test, y_train, y_test = train_test_split(X, y) regressor = make_pipeline( # FeatureExtraction RandomForestRegressor(), ) regressor.fit(X_train, y_train) regressor.score(X_test, y_test) . Forecasting . (Image source: https://www.sktime.org/en/stable/examples/01_forecasting.html.) . Convert index to pd.DatetimeIndex: . y = y.to_timestamp(freq=&quot;M&quot;) y_train, y_test = temporal_train_test_split(y, test_size=36) . Transforming a regressor into forecaster: . from sktime.forecasting.base import ForecastingHorizon from sktime.forecasting.compose import make_reduction from sktime.forecasting.model_selection import temporal_train_test_split forecaster = make_reduction( regressor, scitype=&quot;time-series-regressor&quot;, window_length=12 ) forecaster.fit(y_train) fh = ForecastingHorizon(y_test.index) y_pred = forecaster.predict(fh) . Autocorrelation and stationarity . from sktime.utils.plotting import plot_correlations plot_correlations( realgdp, lags=36, alpha=0.05, pacf_method=&quot;ywadjusted&quot;, acf_title=&quot;Autocorrelation&quot;, pacf_title=&quot;Partial Autocorrelation&quot;, ); . plot_correlations( infl, lags=36, alpha=0.05, pacf_method=&quot;ywadjusted&quot;, acf_title=&quot;Autocorrelation&quot;, pacf_title=&quot;Partial Autocorrelation&quot;, ); . ARIMA as a strong baseline . ARIMA is an algorithm to find Autoregressive Integrated Moving-Average components and build a time series forecasting model. On its basic form, ARIMA has three main parameters to tune. How to find appropriate parameters for ARIMA (p, d, q)? The Box-Jenkins method was well-known as an approach to take the parameters from analysis on autocorrelation and stationarity. . p -&gt; Autoregressive components (a.k.a lags) | d -&gt; Integrative component (diff) | q -&gt; Moving average components (trend lags) | . But there are lots of other subtypes of ARIMA models, such as SARIMA that takes into account seasonality and many others. . If you find ARIMA an interesting algorithm and want know more about it, there are many great videos online. Here we will use the (famously on R) AutoARIMA method, restricting parameters to avoid overfitting. . from sktime.forecasting.arima import AutoARIMA from sktime.forecasting.naive import NaiveForecaster y = load_airline() # ARIMA forecaster = AutoARIMA(sp=12, suppress_warnings=True) forecaster.fit(y) print(f&quot;ARIMA info: n{forecaster.get_fitted_params()}&quot;) y_pred = forecaster.predict(fh=np.arange(1, 13)) # forecast the next 12 months at once # vs NaiveForecaster naive_forecaster = NaiveForecaster(strategy=&#39;last&#39;, sp=12) naive_forecaster.fit(y) y_pred_naive = naive_forecaster.predict(fh=np.arange(1, 13)) fig, ax = plt.subplots(nrows=2, figsize=(15,6)) ax[0].grid() plot_series(y, y_pred, ax=ax[0]) ax[1].grid() plot_series(y, y_pred_naive, ax=ax[1]); . ARIMA info: {&#39;ma.L1&#39;: -0.3634460835424699, &#39;ar.S.L12&#39;: -0.12386440506599077, &#39;ar.S.L24&#39;: 0.19105747581116445, &#39;sigma2&#39;: 130.4479953547119, &#39;order&#39;: (0, 1, 1), &#39;seasonal_order&#39;: (2, 1, 0, 12), &#39;aic&#39;: 1019.1780567487451, &#39;aicc&#39;: 1019.4955170662055, &#39;bic&#39;: 1030.6788460415498, &#39;hqic&#39;: 1023.8513413902231} . Other decision-making baselines . Interpretation Forecaster sktime . &quot;Tomorrow will be just like today&quot; | NaiveForecaster(strategy=&#39;last&#39;) | . &quot;Tomorrow will be close to the overall mean&quot; | NaiveForecaster(strategy=&#39;mean&#39;) | . &quot;Tomorrow will be the mean of the last three days&quot; | NaiveForecaster(strategy=&#39;mean&#39;, window_length=3) | . &quot;Next month will be as it was the same month of last year&quot; | NaiveForecaster(strategy=&#39;last&#39;, sp=12) | . Forecast evaluation workflow in a nutshell . from sktime.forecasting.model_selection import temporal_train_test_split from sktime.forecasting.base import ForecastingHorizon y = load_airline() y_train, y_test = temporal_train_test_split(y, test_size=36) # plotting for illustration plot_series(y_train, y_test, labels=[&quot;y_train&quot;, &quot;y_test&quot;]) print(f&quot;Train: {y_train.shape[0]} points nTest: {y_test.shape[0]} points&quot;) . Train: 108 points Test: 36 points . fh = ForecastingHorizon(y_test.index, is_relative=False) . from sktime.forecasting.compose import AutoEnsembleForecaster from sktime.forecasting.naive import NaiveForecaster from sktime.forecasting.trend import PolynomialTrendForecaster, STLForecaster from sktime.forecasting.exp_smoothing import ExponentialSmoothing from sklearn.metrics import mean_squared_error, r2_score forecasters = [ # (&quot;trend&quot;, STLForecaster(sp=12)), # (&quot;poly&quot;, PolynomialTrendForecaster(degree=1)), (&quot;expm&quot;, ExponentialSmoothing(trend=&quot;add&quot;)), # (&quot;naive&quot;, NaiveForecaster()), ] forecaster = AutoEnsembleForecaster(forecasters=forecasters) forecaster.fit(y=y_train, fh=fh) y_pred = forecaster.predict() # Compute performance metrics metrics = { &#39;mean_squared_error&#39;: mean_squared_error(y_test, y_pred), &#39;r2_score&#39;: r2_score(y_test, y_pred) } print(metrics) # plotting fig, ax = plt.subplots(figsize=(15,6)) plot_series(y_train, y_test, y_pred, labels=[&#39;y&#39;, &#39;y_test&#39;, &#39;y_pred&#39;], ax=ax); . {&#39;mean_squared_error&#39;: 7791.631326444609, &#39;r2_score&#39;: -0.27349496616259006} . Checking a strong baseline . from sktime.forecasting.arima import AutoARIMA from sktime.performance_metrics.forecasting import mean_absolute_percentage_error forecaster = AutoARIMA(sp=12, suppress_warnings=True) forecaster.fit(y_train) y_pred = forecaster.predict(fh = fh) plot_series(y_train, y_test, y_pred, labels=[&quot;y_train&quot;, &quot;y_test&quot;, &quot;y_pred&quot;]) # computing the forecast performance metrics = { &#39;mean_squared_error&#39;: mean_squared_error(y_test, y_pred), &#39;r2_score&#39;: r2_score(y_test, y_pred) } print(metrics) . {&#39;mean_squared_error&#39;: 489.8359037668583, &#39;r2_score&#39;: 0.9199392872227382} . forecasters = [ (&quot;trend&quot;, STLForecaster(12)), # (&quot;poly&quot;, PolynomialTrendForecaster(degree=1)), (&quot;expm&quot;, ExponentialSmoothing(trend=&quot;add&quot;)), # (&quot;naive&quot;, NaiveForecaster(strategy=&quot;last&quot;, sp=12)), ] forecaster = AutoEnsembleForecaster(forecasters=forecasters) forecaster.fit(y_train) y_pred = forecaster.predict(fh = fh) plot_series(y_train, y_test, y_pred, labels=[&quot;y_train&quot;, &quot;y_test&quot;, &quot;y_pred&quot;]) # computing the forecast performance metrics = { &#39;mean_squared_error&#39;: mean_squared_error(y_test, y_pred), &#39;r2_score&#39;: r2_score(y_test, y_pred) } print(metrics) . {&#39;mean_squared_error&#39;: 619.8883331411145, &#39;r2_score&#39;: 0.8986830050391579} . Take aways . Interpolation, Regression and Forecasting are techniques that use diferent methods to make predictions; | Our world is chaotic thus your time series forecasting task may be more complex (multivariate, etc); | Model evaluation is crucial, including baseline analysis; | Validation may require the support of a domain expert that can interpret the results; | Applications can build expert systems (agents) that use predictions to act automaticaly (predictive controllers). | . Themes for additional discussion . Model-centric improvements: Machine learning, Deep learning, AutoML; | Ethics, transparency, reprodutibility and interpretability; | ... | .",
            "url": "https://adaj.github.io/blog/2022/06/06/Introduction-of-time-series-forecasting-with-sktime.html",
            "relUrl": "/2022/06/06/Introduction-of-time-series-forecasting-with-sktime.html",
            "date": " • Jun 6, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Introduction of zero-shot classification using chat data",
            "content": "In this notebook, I would like to make a rapid introduction to zero-shot classification for practitioners. We will cover what it is and explore a use case. After being positively surprised with my toy examples, I prepared this notebook. I wanted to share what I see as a potentially game-changing technique to apply machine learning when ground-truth labels are unavailable or are costly to be collected. . This is not a comprehensive tutorial, nor does it discuss efficient approaches. Still, it&#39;s more like a taste of zero-shot classification in practice for those with no experience with it. In the conclusion, I mention some takeaways that I see on how zero-shot classification cal help in feature engineering pipelines, since you can use its output of intermediary variables to other systems. These intermediary variables are arguably more explainable since you chose the candidate labels, and you can use them in simpler models that other people can scrutinize. . For now, go to the example #1 below! . Imports and utilities . from transformers import AutoTokenizer, pipeline from transformers import AutoModelForSeq2SeqLM from transformers import AutoModelForSequenceClassification def zeroshot_classifier(): # Only works with English text tokenizer = AutoTokenizer .from_pretrained(&quot;facebook/bart-large-mnli&quot;) model = AutoModelForSequenceClassification .from_pretrained(&quot;facebook/bart-large-mnli&quot;) return pipeline(task=&#39;zero-shot-classification&#39;, model=model, tokenizer=tokenizer) # we will need in the second example def translator(src: str, dest: str): src = src.lower() dest = dest.lower() tokenizer = AutoTokenizer .from_pretrained(f&quot;Helsinki-NLP/opus-mt-{src}-{dest}&quot;) model = AutoModelForSeq2SeqLM .from_pretrained(f&quot;Helsinki-NLP/opus-mt-{src}-{dest}&quot;) return pipeline(task=&#39;translation&#39;, model=model, tokenizer=tokenizer) . Example #1: Reasoning or personal impression? . This is an example taken from Fiacco &amp; Rosé (2018). . Suppose you want to classify a text as a causal reasoning, a evaluation reasoning or a personal impression. . If someone says . &quot;Use of coal increases pollution&quot;, we expect the label to be causal reasoning. | &quot;Use of wind power may not be reliable throughtout the year&quot;, we expect the label to be evaluation reasoning. | &quot;I prefer coal power&quot;, we expect the label to be personal impression. | . Of course, these are not completely exclusive classes and could be better conceived to be exclusive, but suppose these are exclusive. . We can use a pre-trained model that is able to perform zero-shot learning and generate labels without any previous training data: . pipe = zeroshot_classifier() candidate_labels = [&#39;causal reasoning&#39;, &#39;evaluation reasoning&#39;, &#39;personal impression&#39;] input_texts = [&quot;Use of coal increases pollution&quot;, &quot;Use of wind power may not be reliable throughout the year&quot;, &quot;I prefer coal power&quot;] with torch.no_grad(): predictions = pipe(input_texts, candidate_labels=candidate_labels) . Okay, I want to know more. What is zero-shot classification? . To be more accurate, zero-shot classification refers to the inference part of zero-shot learning, which is currently a hot research topic in ML literature of transer learning. . As good overview of what this is, Ian Goodfellow&#39;s wrote in Quora the following answer. . Zero-shot learning is being able to solve a task despite not having received any training examples of that task. . He answered the question quite some time ago, so this is not a new thing. In Wikipedia, you will see that this has been researched at least since 2008. . Further, Joe Davison wrote a comprehensive blog post about zero-shot learning, where he greatly explains successes in the field of transfer learning that allowed models to perform surprisingly well in several classification tasks. . Why I have not hearded about this before? . A more accessible use of zero-shot classification (inference) for NLP practitioners has been materialized more recently particularly by the HuggingFace&#39;s transformers library and the models page. New models are being proposed given that more interesting, robust, and semantically transferable datasets are available. More people are using pre-trained models and engaging in transfer learning pipelines. . Okay, but labeling without training samples is really doable? . Zero-shot learning is a particular form of transfer learning. That there are different ways to do the job, and techniques vary in computer vision and NLP. Of course, I still want to see more studies discussing interrater reliability with these kind of tools in a diverse set of scenarios to have a stronger argument on using it &quot;in the wild&quot;. . In this notebook, we will walk through how you can put your hands in some unlabeled text data and label it automatically using models available from HuggingFace&#39;s transformers. . Do you know exactly how/why does this work? . I guess this answer depends a bit on the model you are using. Below, we test the BART model trained by facebook, and I suggest you read the description of the model we are using here. They answer this question quite clearly, but you must have known what &quot;NLI&quot; is. . If you want to read my own (shorter) explanation of their explanation, here it is: . NLI stands for Natural Language Inference, and it refers to a particular classification task. Suppose two pieces of texts, a premise, and a hypothesis. The option of labels are entailment (when the hypothesis confirms the premise), contradiction (when the hypothesis denies the premise), and neutral. Check some examples [here]. There are some NLI datasets available; for example, SNLI and MultiNLI are the most famous ones I know of. Some people also call NLI&#39;s task the entailment classification task. . The MultiNLI dataset is enormous and enables robust algorithm architectures to produce very good models that can be transferred to other semantically similar tasks. . But how does models trained on NLI works for zero-shot classification? . With the entailment, neutral, and contradiction classes to match two pieces of text (premise and hypothesis), a lot of other classification tasks can be adapted to be semantically similar. For example, you can take an arbitrary input text and consider it as the premise. For each item in candidate_labels you want to explore with zero-shot, you create a hypothesis &quot;This sentence is about {item}&quot;. Suppose that your model does a great job at the entailment task. If the model predict that relation as an entailment, that indicates a good match between your input text and this label. Would that make sense? . Let&#39;s go through another example to see it in action again, but in a more challenging context. . Example #2: DOTA-2 in-game chats . If you want to go through a second example, I will use some data from DOTA-2 chats to classify them as one of the following candidate_labels = [&#39;chitchat&#39;, &#39;game features&#39;, &#39;coordination&#39;, &#39;toxic offense&#39;, &#39;gender discrimination&#39;, &#39;religious intolerance&#39;, &#39;racism&#39;]. This data is in Russian, so we have a translation step in between that we may loose some information. Also we are not carrying too much in preprocessing steps, but they are indeed required for more serious projects. Also, it seems that there are zero-shot models in a few other languages available out there, such as French, Spanish, German, even Russian. See a list from HuggingFace models. . a. Load the data . df = pd.read_csv(&#39;/kaggle/input/gosuai-dota-2-game-chats/dota2_chat_messages.csv&#39;, nrows=100) df[&#39;text&#39;] = df[&#39;text&#39;].fillna(&#39;&#39;) print(&#39;Mean length of text&#39;, df[&#39;text&#39;].apply(lambda x: len(x)).mean()) print(df.head(15)) . %%time df_sample = df.sample(50) . b. Translate to English . translate = translator(&#39;ru&#39;, &#39;en&#39;) . %%time with torch.no_grad(): translated_text = translate([t[:100] for t in list(df_sample[&#39;text&#39;])]) df_sample[&#39;text_en&#39;] = [t[&#39;translation_text&#39;] for t in translated_text] . df_sample.head(10) . c. Load zero-shot classifier from Huggingface&#39;s transformers library . candidate_labels = [&#39;chitchat&#39;, &#39;game features&#39;, &#39;coordination&#39;, &#39;toxic offense&#39;, &#39;gender discrimination&#39;, &#39;religious intolerance&#39;, &#39;racism&#39;] pipe = zeroshot_classifier() . d. Generate and explore predictions . %%time with torch.no_grad(): predictions = pipe(list(df_sample[&#39;text_en&#39;]), candidate_labels=candidate_labels) predictions[0] . Concluding remarks . In case you want to know more about zero-shot learning, I encourage you to go through the following material: . https://joeddav.github.io/blog/2020/05/29/ZSL.html | https://arxiv.org/abs/1909.00161 | https://www.deeplearningbook.org/contents/representation.html (Section 15.2) | https://www.aaai.org/Papers/AAAI/2008/AAAI08-132.pdf | . As a takeaway, I think a well-designed zero-shot classifier (with suitable candidate labels) can be a game-changing tool for several AI projects. . One use case is, for example, &quot;expert systems,&quot; where you generate these output probabilities for classes that you understand as intermediary features that you provide to a rule-based decision-making mechanism. Then you can write things like &quot;if causal_reasoning is high, do the action A; if evaluation_reasoning is high, do the action B.&quot; There has been some research opportunity with zero-shot learning as a feature engineering step because of this kind of system. Interestingly, one can automate understandable features computed from black-box models. Also, if you build candidate_labels using some theory is great because you have more evidence to back up design choices. . The main issue I see with this is how we can evaluate if the zero-shot labels are suitable. Well, one way to measure is by letting the model compute several labels, and then you or another person can label manually. With your labels and the zero-shot ones, you can use Cohen&#39;s Kappa as your agreement level. That, however, is not scalable when you are testing several candidate_labels. . If you see interesting use cases for zero-shot learning or want to interact/discuss this notebook, please comment below!! .",
            "url": "https://adaj.github.io/blog/2021/12/12/Introduction-of-zero-shot-classification-using-chat-data.html",
            "relUrl": "/2021/12/12/Introduction-of-zero-shot-classification-using-chat-data.html",
            "date": " • Dec 12, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Adelson started his PhD in the Department of Instructional Technology of the University of Twente in 2020, supervised by Prof dr Ton de Jong, Prof dr Susan McKenney, and Dr Pantelis Papadopoulos. . In his doctoral studies, he aims to design and investigate conversational agents that support students talking productively in online science labs. . His bachelor was in Computer Engineering, and masters in Systems &amp; Computing, both at the Federal University of Rio Grande do Norte (Brazil). During his master’s and bachelor’s, he assisted the SmartMetropolis research group and published his first papers on international conferences. He had the opportunity to receive two research grants (2018 and 2019) from the Google LARA initiative (Latin America Research Awards) to support his project on predictive policing. He worked as a data engineer in the Public Ministry of Rio Grande do Norte, managing a data lake infrastructure and developing dashboards to help prosecutors analyze public budget-related issues. . His current research interests are conversational agents, collaborative learning, inquiry-based learning, and ethics in AI. .",
          "url": "https://adaj.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://adaj.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}