{
  
    
        "post0": {
            "title": "Zero-shot classification with DOTA 2 in-game chats",
            "content": "In this notebook, I would like to make a rapid introduction to zero-shot classification for practitioners. We will cover what it is and explore a use case. . This is not a comprehensive tutorial, nor does it discuss efficient approaches. Still, it&#39;s more like a taste of zero-shot classification in practice for those with no experience with it. . After being positively surprised, I prepared this notebook, and I wanted to share what I see as a potentially game-changing technique to apply machine learning when ground-truth labels are not available. . Zero-shot learning, what is that?! . Ian Goodfellow&#39;s answered in Quora (https://qr.ae/pGl1ss). . Zero-shot learning is being able to solve a task despite not having received any training examples of that task. . He answered the question quite some time ago, so this is not a new thing. In computer vision, people have been investigating that since 2008, according to Wikipedia. . In NLP, this is being more materialized recently with advances in transfer learning. New architectures are being proposed and because there are some interesting datasets where bigger models can be pre-trained on. . The most common example you will see out there about zero-shot learning is the zebra example. Suppose you only trained your model with horses. Now you want to classify a zebra, but your model doesn&#39;t know about it. But if you are able to say to the model that &quot;zebra&quot;s (a new label) are striped and horse-like (features), a model able to perform zero-shot learning can correctly identify that zebra. . Okay, but is that possible? . Of course. Zero-shot learning is a particular form of transfer learning. That there are different ways to do the job, and techniques vary in computer vision and NLP. . In this notebook, we will walk through how you can put your hands in some unlabeled text data and label it automatically using models available from HuggingFace&#39;s transformers. . I don&#39;t have too much time. Can you show me this quickly? . Sure, I understand. First, I will show a small demonstration. . If you want to go through a second example, I will use some data from DOTA-2 chats to classify them as one of the following candidate_labels = [&#39;chitchat&#39;, &#39;game features&#39;, &#39;coordination&#39;, &#39;toxic offense&#39;, &#39;gender discrimination&#39;, &#39;religious intolerance&#39;, &#39;racism&#39;]. This data is in Russian, so we have a translation step in between. But it seems that there are zero-shot models in a few other languages available out there, such as French, Spanish, German, even Russian. See a list from HuggingFace models. . Example 1: Reasoning or personal impression? . This is an example taken from Fiacco &amp; Rosé (2018). . Suppose you want to classify a text as a causal reasoning, a evaluation reasoning or a personal impression. . If someone says . &quot;Use of coal increases pollution&quot;, we expect the label to be causal reasoning. | &quot;Use of wind power may not be reliable throughtout the year&quot;, we expect the label to be evaluation reasoning. | &quot;I prefer coal power&quot;, we expect the label to be personal impression. | . Of course, these are not completely exclusive classes and could be better conceived to be exclusive, but suppose these are exclusive. . We can use a pre-trained model that is able to perform zero-shot learning and generate labels without any previous training data: . from transformers import AutoTokenizer, pipeline from transformers import AutoModelForSequenceClassification def zeroshot_classifier(): # Only works with English text tokenizer = AutoTokenizer .from_pretrained(&quot;facebook/bart-large-mnli&quot;) model = AutoModelForSequenceClassification .from_pretrained(&quot;facebook/bart-large-mnli&quot;) return pipeline(task=&#39;zero-shot-classification&#39;, model=model, tokenizer=tokenizer) pipe = zeroshot_classifier() some_texts = [&quot;Use of coal increases pollution&quot;, &quot;Use of wind power may not be reliable throughout the year&quot;, &quot;I prefer coal power&quot;] candidate_labels = [&#39;causal reasoning&#39;, &#39;evaluation reasoning&#39;, &#39;personal impression&#39;] predictions = pipe(some_texts, candidate_labels=candidate_labels) . fig, ax = plt.subplots(nrows=3, figsize=(6,10)) for i, p in enumerate(predictions): sns.barplot(y=&#39;labels&#39;, x=&#39;scores&#39;, data=p, ax=ax[i], order=candidate_labels) ax[i].set_title(p[&#39;sequence&#39;][:50]) fig.tight_layout() . Why does this work? . I suggest you read the description of the model we are using here, they answer this question in very simply. . If you want to read my own (shorter) explanation of their explanation, here it is: . To understand the explanation, you need to know what is a NLI (Natural language inference) dataset. There are a few different NLI datasets out there, for example SNLI and MultiNLI are the most famous ones. In the task of these datasets, there is two pieces of texts, a premise and a hypotehsis. The option of labels are entailment (when the hypothesis confirms the premise), contradiction (when the hypothesis denies the premise), and neutral. Check some examples here. Some people call NLI&#39;s task as the entailment classification task. . The MultiNLI dataset is huge and enables robust algorithm architectures to produce very good models that can be transferred to other semantically similar tasks. . Suppose the model does a great job at the entailment task. Given our set of three candidate_labels, zero-shot learning with can be leveraged in the following way. Take the input text as the premise. For each ith candidate_labels, turn the into &quot;This sentence is about {i}&quot; and that is used as our hypothesis, predict the entailment of this hypothesis. Finally, transform the output to generate a probability for each candidate_label. . Let&#39;s go through another example to see it in action in a harder context. . Example 2: DOTA-2 in-game chats . Disclaimer: The dataset for this example contains text that may be considered profane, vulgar, or offensive. . a. Load the data . df = pd.read_csv(&#39;~/Downloads/dota2_chat_messages.csv&#39;, nrows=100) df[&#39;text&#39;] = df[&#39;text&#39;].fillna(&#39;&#39;) print(&#39;Previous mean length of text&#39;, df[&#39;text&#39;].apply(lambda x: len(x)).mean()) print(&#39;Removing 1% outliers of really big texts...&#39;) max_size_message = int(df[&#39;text&#39;].apply(len).quantile(.99)) df[&#39;text&#39;] = df[&#39;text&#39;].apply(lambda x: x[:max_size_message]) print(&#39;Mean length of text&#39;, df[&#39;text&#39;].apply(lambda x: len(x)).mean(), &#39; n n&#39;) print(df.head(15)) . Previous mean length of text 11.83 Removing 1% outliers of really big texts... Mean length of text 11.72 match time slot text 0 0 1005.12122 9 ладно гг 1 0 1005.85442 9 изи 2 0 1008.65372 9 од 3 0 1010.51992 9 ебаный 4 0 1013.91912 9 мусор на войде 5 0 1800.31402 9 мусор 6 0 1801.71882 9 на войде 7 0 1802.98982 9 репорт 8 0 1808.40822 9 100% 9 1 -131.14018 0 twitch.tv/rage_channel 10 1 -121.60481 0 https://www.twitch.tv/rage_channel 11 1 244.47367 7 2 даша подряд 12 1 249.93900 7 баша 13 1 255.00443 4 где даша? 14 1 261.20293 4 даша домой . df_sample = df.sample(30) . b. Translate to English . from transformers import AutoModelForSeq2SeqLM def translator(src: str, dest: str): src = src.lower() dest = dest.lower() tokenizer = AutoTokenizer .from_pretrained(f&quot;Helsinki-NLP/opus-mt-{src}-{dest}&quot;) model = AutoModelForSeq2SeqLM .from_pretrained(f&quot;Helsinki-NLP/opus-mt-{src}-{dest}&quot;) return pipeline(task=&#39;translation&#39;, model=model, tokenizer=tokenizer) translate = translator(&#39;ru&#39;, &#39;en&#39;) . %%time translated_text = translate([t[:max_size_message] for t in list(df_sample[&#39;text&#39;])]) df_sample[&#39;text_en&#39;] = [t[&#39;translation_text&#39;] for t in translated_text] . CPU times: user 2min 28s, sys: 1min 25s, total: 3min 54s Wall time: 3min 7s . df_sample.head(10) . match time slot text text_en . 25 2 | 1248.42850 | 0 | yes dog | yes dog | . 56 3 | 2371.22080 | 6 | я говор. | I&#39;m a talker. | . 22 1 | 716.05853 | 6 | стример харду сливает)0)) | strimer hardu drips off)0)) | . 59 3 | 2516.98400 | 2 | ) | ) | . 62 4 | -38.79053 | 1 | профиль глянь :D | Look at the profile:D | . 27 2 | 1281.95360 | 4 | HAHAH | HAHAH | . 15 1 | 597.98733 | 4 | долбоеб сука на дизрапторе | Fucking bitch on disraptor. | . 57 3 | 2376.30480 | 6 | У тебя мать портовая шлюха | Your mother&#39;s a port whore. | . 44 3 | 145.63110 | 0 | ты это антимагу написаЛ? | Did you write that anti-maga? | . 38 2 | 2263.90490 | 4 | COMMEND ME TY | COMMEND ME TY | . c. Load zero-shot classifier from Huggingface&#39;s transformers library . pipe = zeroshot_classifier() . d. Define your candidate labels . candidate_labels = [&#39;chitchat&#39;, &#39;game features&#39;, &#39;coordination&#39;, &#39;toxic offense&#39;, &#39;gender discrimination&#39;, &#39;religious intolerance&#39;, &#39;racism&#39;] . e. Generate and explore predictions . %%time predictions = pipe(list(df_sample[&#39;text_en&#39;]), candidate_labels=candidate_labels) predictions[0] . CPU times: user 4min 6s, sys: 28.4 s, total: 4min 34s Wall time: 4min 34s . {&#39;sequence&#39;: &#39;yes dog&#39;, &#39;labels&#39;: [&#39;coordination&#39;, &#39;chitchat&#39;, &#39;toxic offense&#39;, &#39;game features&#39;, &#39;racism&#39;, &#39;religious intolerance&#39;, &#39;gender discrimination&#39;], &#39;scores&#39;: [0.5105039477348328, 0.14312249422073364, 0.11186554282903671, 0.07289346307516098, 0.06860581040382385, 0.05673101544380188, 0.03627773001790047]} . labels = [] for p in predictions: labels.append(p[&#39;labels&#39;][np.argmax(p[&#39;scores&#39;])]) df_sample[&#39;label&#39;] = labels df_sample[[&#39;text&#39;, &#39;text_en&#39;, &#39;label&#39;]] . text text_en label . 25 yes dog | yes dog | coordination | . 56 я говор. | I&#39;m a talker. | chitchat | . 22 стример харду сливает)0)) | strimer hardu drips off)0)) | coordination | . 59 ) | ) | chitchat | . 62 профиль глянь :D | Look at the profile:D | chitchat | . 27 HAHAH | HAHAH | chitchat | . 15 долбоеб сука на дизрапторе | Fucking bitch on disraptor. | toxic offense | . 57 У тебя мать портовая шлюха | Your mother&#39;s a port whore. | toxic offense | . 44 ты это антимагу написаЛ? | Did you write that anti-maga? | gender discrimination | . 38 COMMEND ME TY | COMMEND ME TY | coordination | . 4 мусор на войде | There&#39;s garbage in the door. | toxic offense | . 81 сам прочитай что написал | Read what you wrote. | coordination | . 26 lul | lol | chitchat | . 10 https://www.twitch.tv/rage_channel | https://www.twitch.tv/range_channel | coordination | . 96 ах ты крыса мелкая | You little rat. | toxic offense | . 11 2 даша подряд | 2 ducks in a row | coordination | . 76 у нас фидер | We&#39;ve got a fider. | coordination | . 97 Пизда тебе | You&#39;re a pussy. | gender discrimination | . 28 yeah | Yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah... | chitchat | . 37 lol | lol | chitchat | . 2 од | - Yeah, yeah, yeah, yeah, yeah, yeah, yeah, ye... | chitchat | . 58 А СЫН ЕБАЛ ТЕБЯ В РОТ )))) | And your son wanted you in the horn ) ) . . . ... | coordination | . 64 тебе всё равно фиаско братан) | You&#39;re still a fiasco, bro. | toxic offense | . 86 играю | I&#39;m playing. | game features | . 9 twitch.tv/rage_channel | Twitch.tv/range_channel | chitchat | . 0 ладно гг | Okay. | chitchat | . 77 а вы пузу | And you&#39;re the belly. | coordination | . 32 no idiot | no idiot | chitchat | . 71 боже | Oh, my God. | toxic offense | . 84 маленький | small | chitchat | . how_many_to_plot = 10 fig, ax = plt.subplots(nrows=how_many_to_plot, figsize=(6,30)) for i, p in enumerate(random.sample(predictions, how_many_to_plot)): sns.barplot(y=&#39;labels&#39;, x=&#39;scores&#39;, data=p, ax=ax[i], order=candidate_labels) ax[i].set_title(p[&#39;sequence&#39;][:50]) fig.tight_layout() . In case you want to know more about zero-shot learning, I encourage you to go through the following material: . https://joeddav.github.io/blog/2020/05/29/ZSL.html | https://arxiv.org/abs/1909.00161 | https://www.deeplearningbook.org/contents/representation.html (Section 15.2) | https://www.aaai.org/Papers/AAAI/2008/AAAI08-132.pdf | . As a takeaway, I think a well designed zero-shot classifier (with good candidate labels) can be game-changing tool for several AI projects. . One use case is for example &quot;expert systems&quot;, where you generate these output probabilities for classes that you understand as intermediary features that you provide to a rule-based decision-making mechanism. Then you are able to write things like &quot;if causal_reasoning is high, do the action A; if evaluation_reasoning is high do the action B&quot;. I think there has some research opportunity with zero-shot learning as a feature engineering step because with this kind of system, although features are computed from black box models, your features are abstractions that you can understand and explain. Also, if you build candidate_labels using some theory, that is great for you because you have more evidence to back up your design. . The main issue I see with this is how can we evaluate if the zero-shot labels are good. Well, one way to measure is by letting the model compute several labels, and then you or another person can label manually. With your labels and the zero-shot ones, you can use Cohen&#39;s Kappa as your agreement level. That, however, is not scalable when you are testing several candidate_labels. . If you see interesting use cases for zero-shot learning, or want to interact/discuss about this notebook, please comment below!! .",
            "url": "https://adaj.github.io/blog/2021/11/27/Zero-shot-classification-with-DOTA-2-in-game-chats.html",
            "relUrl": "/2021/11/27/Zero-shot-classification-with-DOTA-2-in-game-chats.html",
            "date": " • Nov 27, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://adaj.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://adaj.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}